#+TITLE: Herbstrith' Oblivious Nbody Lab Book
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Herbstrith(H) noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

* Study materials and theories
** Cuda
   What every CUDA Programmer Should Know About OpenGL
   http://www.nvidia.com/content/gtc/documents/1055_gtc09.pdf
   On Cuda usage of recursive kernels ( Dynamic Parallelism is only avaiable for devices with a compute capability >= 3.5)
   http://developer.download.nvidia.com/assets/cuda/files/CUDADownloads/TechBrief_Dynamic_Parallelism_in_CUDA.pdf
   http://devblogs.nvidia.com/parallelforall/cuda-dynamic-parallelism-api-principles/
   Classic
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
   Dynamic Parallelism simple example
   http://www.fz-juelich.de/SharedDocs/Downloads/IAS/JSC/EN/slides/advanced-gpu/adv-gpu-dynpar.pdf?__blob=publicationFile
   Cuda C Programming Guide
   http://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf  ( Appendix C, pg 140, for dynamic parallelism)

** Cache oblivious nbody
  + Presentation
    https://github.com/CppCon/CppCon2014/blob/master/Presentations/ -> Decomposing a problem

** GEMS3 nbody
http://http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html

* <03-12-2015> Algorithm study                                   :Herbstrith:
  One thing to note is that the algorithm idea is to have less RAM memory access, taking advantage on data locality.
  As GPU uses GDDR5 RAM, we won't have such an increase in performance as the cpu counterpart. Still data locality is a good thing to experiment on.
  We are also adding recursion to the algorithm, which the gpu doesn't quite like, and a thread seems to do more work, again because the algorithm was made thinking on cpus.
* <04-12-2015> Algorithm study                                   :Herbstrith:
An excerpt from the http://http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html article:
"
We may think of the all-pairs algorithm as calculating each entry f ij in an NxN grid of all pair-wise forces.[1] Then the total force F i
(or acceleration a i ) on body i is obtained from the sum of all entries in row i. Each entry can be computed independently, so there is O(N 2)
available parallelism. However, this approach requires O(N 2) memory and would be substantially limited by memory bandwidth. Instead, we serialize
some of the computations to achieve the data reuse needed to reach peak performance of the arithmetic units and to reduce the memory bandwidth required.

Consequently, we introduce the notion of a computational tile, a square region of the grid of pair-wise forces consisting of p rows and p columns. 
Only 2p body descriptions are required to evaluate all p 2 interactions in the tile (p of which can be reused later). These body descriptions can be
stored in shared memory or in registers. The total effect of the interactions in the tile on the p bodies is captured as an update to p acceleration
vectors.

To achieve optimal reuse of data, we arrange the computation of a tile so that the interactions in each row are evaluated in sequential order, updating
the acceleration vector, while the separate rows are evaluated in parallel.

"
This is a very similar approach to the one proposed in the cache-oblivious algorithm. On both we have the parallel tile notion, and both aim to make reuse of the data.
Also this one does take in account the gpu architecture.
* <07-12-2015> Meeting Prof Lucas M. Schnorr                     :Herbstrith:
I was wrong, the algorithm is still a valid idea. My mistake was focusing on the word "Cache", while the algorithm does take advantage of
cache locality, it also does manage to do only half the calculations (thanks to Newtons third law, represented by the add_force function). 
* <08-12-2015> Code from the Decomposing a problem presentation on which we will work
 #+BEGIN_SRC c
 // calculate_forces kernel function ... here is where we start
__global__ void calculate_forces( int nbodies, Body *bodies)
{
  triangle(0, nbodies, bodies);
}
#+END_SRC
Not much to change here

#+BEGIN_SRC c

// triangle kernel function
__device__ void triangle(int n0, int n1, Body *bodies)
{
  int dn = n1 - n0;
  if(dn > 1){
    int nm = n0 + dn/2;

    triangle(n0, nm,bodies);  //spawn new thread
    triangle(nm,n1,bodies);

    rect(n0,nm,nm,n1,bodies);  //spawn new thread
  }
}
#+END_SRC

The triangle's and rect calls would only work on a dynamic parallelism capable device, else we
we cant spawn more threads to do the work.

#+BEGIN_SRC c

// rectangle kernel function ( without coarsening )
__device__ void rect(int i0, int i1, int j0, int j1, Body *bodies)
{
  int di = i1 - i0;
  int dj = j1 -j0;

  if(di > 1 && dj >1){
    int im = i0 + di/2;
    int jm = j0 + dj/2;

    rect(i0, im, j0, jm, bodies); //spawn new threads
    rect(im, i1, jm, j1, bodies);

    rect(i0, im, jm, j1, bodies); //spawn new threads
    rect(im, i1, j0, jm, bodies); //spawn new threads
    
  } else {
    if (di > 0 && dj >0){
      double fx, fy;
      calculate_force(&fx, &fy, bodies[i0], bodies[j0]);
      add_force(&bodies[i], fx, fy);
      add_force(&bodies[j], -fx, -fy);
    }
  }
}
#+END_SRC

Same as the triangle function

#+BEGIN_SRC c

//calculate_force: body x body interaction
__device__ void calculate_force(double *fx, double *fy, const Body &bi, const Body &bj)
{ 
  double dx = bj.x - bi.x;
  double dy = bj.y - bi.y;
  double dist2 = dx * dx + dy * dy;  //distance squared
  double dist = std::sqrt(dist2);
  double f = bi.mass * bj.mass * GRAVITY / dist2;
  *fx = f * dx / dist;
  *fy = f *dy / dist;
}

// add_force kernel function
__device__  void add_force(Body* b, double fx, double fy)
{
  b->xf += fx;
  b->yf += fy;
}

#+END_SRC
These shouldnt change much.

 What we would want is a proper way to fit these rectangles and triangles on the 
threads and thread blocks. This would give us an excelent boost on performance.
We also woll try the naive way by making use of the dynamic parallelism, but this
approach would have some overhead, and wouldn't have a good result as the first.
* <10-12-2015> Cuda Implementation
** Dynamic Parallelism
 We shall start by making a version that makes use of the Dinamyc Parallelism, which should be easier to implement.
 This version will be very similar to the Cache Oblivious implementation, so we might not have as much as an increase
 performance.
 We will represent the data in a similar fashion to the GEMS3 nbody, so we can compare to it on a first analisys.
** Cuda Native
 This version we will try to take a new approach to the algorithm, taking in consideration the GPU architecture.
 One first idea is to move diagonally in the grid at each iteration. This would lead to a low work load on the first
 iterations and increase it as the computation goes. We could also think of ways of getting work from the "middle"
 tiles, trying to have more  work on the first iterations.
 
* <19-12-2015> Running on Orion
 Some many problems appeared here.
 In the end, to compile i used the following commands:
 nvcc -arch=sm_35 -rdc=true -c nbody_kernel.cu
 nvcc -arch=sm_35 -dlink -o nbody_kernel_link.o nbody_kernel.o -lcudadevrt -lcudart
 g++ nbody_kernel.o nbody_kernel_link.o main.cpp -L/usr/local/cuda/lib64/ -lcudart -lcudadevrt

 Also needed to set export LD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64:$LD_LIBRARY_PATH
 But still got the following error: FATAL: Error inserting nvidia_340_uvm : invalid argument

 After some more cuda exploring, i managed to run a dynamic paralel example on my gtx 760. So i can
 use my machine for compiling and running the code ( i tought the error's i had received were due to
 the gtx 760 not having a  3.5 cc). This same code would generate the same error as mentioned before
 on the orion machine. Now we need to fix this.
 
 And an approach to compile using just a Makefile can was found here:
 http://stackoverflow.com/questions/28719927/dynamic-parallelism-separate-compilation-undefined-reference-to-cudaregiste
* <21-12-2015> Cuda and the debug process
 Learning the Cuda way to error checking is  extremelly important, as error's due to cuda calls are silent otherwise.
 http://codeyarns.com/2011/03/02/how-to-do-error-checking-in-cuda/
 https://code.google.com/p/stanford-cs193g-sp2010/wiki/TutorialWhenSomethingGoesWrong
 
 cuda-memcheck is a helpfull tool for debugging
 cuda-gdb requires 2 gpu to work or not using the gpu on the system graphic interface
* <23-12-2015> Cuda compilation on gtx 760
 So it does matter the architecture version on the compilation. Even thought the program does compile
 when making the cuda calls during the program execution we will have silent errors. One thing to keep
 in mind is that even when using the proper cuda error checking we just receive an "Unknown error" message
 , due to compiling for the wrong architecture.
* <30-12-2015> Cuda compilation on k20
After a week trying many makefile configurations and cmake configurations. Nothing worked.
 But then after trying to run a simple code, even that didn't worked. After all
 the problem is in the machine configuration it seems. Lessons learned: cuda compilation
 aint that complicated and the developer MUST make use of an error checking function on his code.
* <31-12-2015> No recursion Algorithm 
 An approach to the Half Work idea but without the recursion.
 We have to move diagonally, so we need to feed our threads blocks a contiguous diagonal list
 so that they can be all computated on parallel. For that we will use a component (function) that will return
 a Vector4 that will return the line dimensions when a block asks. For every time the function is update we have
 to atomically update it's value as well (Affect the  performance). 
 
* <01-01-2016> Implementing the Algorithm
I'll use the cuda Gems3 nbody code and just change the actuall kernel.
This will also simplificate the performance analisys and make it more accurate, since
we will be using the same data structures and initialization.
* ERAD 2016 Paper
DEADLINE: <2016-01-08 Sex>
** TODO Have Working version of the half work
** TODO Performance Analysis runs
** TODO Write
Couldn't meet the deadline.

* <04-01-2016> No recursion Algorithm Second approach
 Instead of using an atomic diagonal manager we can simply use the blockIdx.
 I ended up on this formula:
#+BEGIN_SRC C
 workRange.x = deviceNumBodies - diagonalCounter_i;
 workRange.y = deviceNumBodies -diagonalCounter_i;    
 workRange.z =  (blockSize * blockIdx.x) +(numBlocks*blockSize*diagonalCounter_j) ;
 workRange.w = blockSize * blockIdx.x + blockSize + (numBlocks*blockSize*diagonalCounter_j);
#+END_SRC

 This way every block on the gpu will grab a chunk of the diagonal line to work on it. We also dont need to have a 
 mutex between the blocks, since every block will grab a chunk of the diagonal line
 and, if they finish their work, they can grab other chunk of the line without interfering with adjacent blocks.
 But the workRange update must be done only by one thread inside the block, since there is a race condition in the
 workRange update, so we have a slowdown at this point. 
 Also every block must change the line together. Therefore if a block end his work early, we will have a busy waiting
 at this point. Like the workRange update, only one block can update the lineCounter, for we also have a race condition.
 
 Also our function to calculate the force differs from the gpu Gems.
 
#+BEGIN_SRC C
 // work on the tile
  for (int tile_i = 0; tile_i < numTiles; tile_i++)
  {
    int index_i = (index.x - tile_i - index.y)*deviceNumBodies + (index.y  + tile_i);
    typename vec4<T>::Type pos_i = oldPos[index_i];
    
    for (int tile_j = tile_i+1; tile_j < numTiles; tile_j++) 
    {
      int index_j = (index.x - tile_j - index.y)*deviceNumBodies + (index.y  + tile_j);
      typename vec4<T>::Type pos_j = oldPos[index_j];

      typename vec3<T>::Type accel = {0.0f, 0.0f, 0.0f};

      //aceleration, body 1 , body 2
      accel = bodyBodyInteraction<T>(accel, pos_i, pos_j);

      // acceleration = force / mass;
      // new velocity = old velocity + acceleration * deltaTime
      // note we factor out the body's mass from the equation, here and in bodyBodyInteraction
      // (because they cancel out).  Thus here force == acceleration
      typename vec4<T>::Type velocity = vel[deviceOffset + index_i];
    
      velocity.x += accel.x * deltaTime;
      velocity.y += accel.y * deltaTime;
      velocity.z += accel.z * deltaTime;

      velocity.x *= damping;
      velocity.y *= damping;
      velocity.z *= damping;

      // new position = old position + velocity * deltaTime
      pos_i.x += velocity.x * deltaTime;
      pos_i.y += velocity.y * deltaTime;
      pos_i.z += velocity.z * deltaTime;
    
      // we also update the second particle with the inverse force (velocity) by the principle of newtons third law
      pos_j.x += -velocity.x * deltaTime;
      pos_j.y += -velocity.y * deltaTime;
      pos_j.z += -velocity.z * deltaTime;
    }
    
#+END_SRC

 Where every thread gets a chunk (size NumTile) of the block work. Here is where we also end up losing performance, all due to
 the need of updating the position and velocity at every iteration (which works best in the cpu approach which was the proposal).

** Performance Analysys (Quick&dirty)
 With a rude implementation of this idea, and after some sample runs we end up with very poor results:
 With a numbodies of 1000 and 10 iterations:

 Gpu Gems original implementation: average on 0.24 ms per iteration
 Half work prototype implementation: average on 1.40 ms per iteration

 Even though we are not properly distributing the work on threads ... at most we could optimize the work by half
 still gettin on about 0.7 ms which is still orders of magnitude slower.
 
 
* <09-01-2016> No recursion Algorithm Memory management
A thread block will allways use most of the same J bodies throught the execution. These could be loaded on shared memory to achieve
best performance.

* <10-01-2016> On the Half Work Algorithm efficiency
** All-pair
*** N iterations
The all-pairs nbody problem is almost an embarrassingly parallel problem, so one could assume (if such parallel power would be avaiable) it takes N iterations to finish it.

** Half-Work
*** N iterations
Now the half-work approach isn't as parallelizable as the all-pair. Assuming we could process an entire diagonal line parallely, we
still would take N steps to finish it.
*** Less work
The advantage over the all-pairs algorithm would be that at each iteration
we would have less and less work to do each step. Also, not so advantageous on a gpgpu.
*** Synchronization overhead
There's the need for barriers and mutexes for race conditions. Highly undesired on a gpgpu.
The barriers are needed on both the recursive and no-recursive approachs. 
On the recursive approach we would also have all the Kernel launch's (dynamic parallelism) overhead.
*** No coalesced memory access
Couldn't find a efficient way to access the memory on the no-recursive approach. 
The original approach based on recursion has the property of being cache-friendly.
But the recursive approach was designed and tested on a CPU which has more cache memory at it's disposal.

* <11-01-2016> No recursion performance
Decomposing the execution of the half work algorithm showed us that:
Total Synchronization overhead (Removed all the calculation): 0.855 ms average time on 1000 particles
Parcial Synchronization overhead ( removed all the calculation and the between blocks sync ): 0.272 ms average time on 1000 particles
Gems3 original: 0.107 ms average time on 1000 particles

Conclusion: huge bottleneck on the synchronization step. Can't see any way to overpass this, thus rendering
this approach not efficient.

* <11-01-2016> On dynamic parallelism
http://ppomorsk.sharcnet.ca/CSE746/lecture6_CSE746_2014.pdf
http://users.ece.gatech.edu/~sudha/academic/class/ece8823/Lectures/Module-6-Microarchitecture/cuda-dyn-par.pdf

http://devblogs.nvidia.com/parallelforall/a-cuda-dynamic-parallelism-case-study-panda/
This last one presents two statements that are relevant to our work (What Dynamic Parallelism is Not section):

"With all the advantages of Dynamic Parallelism, however, it is also important to understand when not to use it.
In all of the examples in this series of posts, when a child kernel is launched, it always consisted of a large
number of threads, and also of a large number of thread blocks. It is possible in some cases for each child grid 
to consist of a single thread block, which still contains O(100) threads. However, it is a bad idea to start a child
grid with just a few threads. This would lead to severe under-utilization of GPU parallelism, since only a few
threads in a warp would be active, and there would not be enough warps. This would also lead to GPU execution
dominated by kernel launch latencies."

Which would be just like our case where we will start with one single thread calling the Triangle() function and 
every other thread spawning 3 other threads and so on.

"We can now see that Dynamic Parallelism is useful when it results in fat but relatively shallow trees. Adaptive
 grid algorithms often result in such trees. Traditional tree processing, in contrast, usually involves trees with
 small node degree but large depth, and is not a good use case for dynamic parallelism."

Our tree will not be shallow since it needs to  have enough chunks to be computaded in parallel.
Adding this to the synchronization need, doesnt lead to a good expectation.

