#+TITLE: Herbstrith' Oblivious Nbody Lab Book
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Herbstrith(H) noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

* Study materials and theories
** Cuda
   What every CUDA Programmer Should Know About OpenGL
   http://www.nvidia.com/content/gtc/documents/1055_gtc09.pdf
   On Cuda usage of recursive kernels ( Dynamic Parallelism is only avaiable for devices with a compute capability >= 3.5)
   http://developer.download.nvidia.com/assets/cuda/files/CUDADownloads/TechBrief_Dynamic_Parallelism_in_CUDA.pdf
   http://devblogs.nvidia.com/parallelforall/cuda-dynamic-parallelism-api-principles/
   Classic
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
   
** Cache oblivious nbody
  + Presentation
    https://github.com/CppCon/CppCon2014/blob/master/Presentations/ -> Decomposing a problem
* <03-12-2015> Algorithm study                                   :Herbstrith:
  One thing to note is that the algorithm idea is to have less RAM memory access, taking advantage on data locality.
  As GPU uses GDDR5 RAM, we won't have such an increase in performance as the cpu counterpart. Still data locality is a good thing to experiment on.
  We are also adding recursion to the algorithm, which the gpu doesn't quite like, and a thread seems to do more work, again because the algorithm was made thinking on cpus.
* <04-12-2015> Algorithm study                                   :Herbstrith:
An excerpt from the http://http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html article:
"
We may think of the all-pairs algorithm as calculating each entry f ij in an NxN grid of all pair-wise forces.[1] Then the total force F i
(or acceleration a i ) on body i is obtained from the sum of all entries in row i. Each entry can be computed independently, so there is O(N 2)
available parallelism. However, this approach requires O(N 2) memory and would be substantially limited by memory bandwidth. Instead, we serialize
some of the computations to achieve the data reuse needed to reach peak performance of the arithmetic units and to reduce the memory bandwidth required.

Consequently, we introduce the notion of a computational tile, a square region of the grid of pair-wise forces consisting of p rows and p columns. 
Only 2p body descriptions are required to evaluate all p 2 interactions in the tile (p of which can be reused later). These body descriptions can be
stored in shared memory or in registers. The total effect of the interactions in the tile on the p bodies is captured as an update to p acceleration
vectors.

To achieve optimal reuse of data, we arrange the computation of a tile so that the interactions in each row are evaluated in sequential order, updating
the acceleration vector, while the separate rows are evaluated in parallel.

"
This is a very similar approach to the one proposed in the cache-oblivious algorithm. On both we have the parallel tile notion, and both aim to make reuse of the data.
Also this one does take in account the gpu architecture.
* <07-12-2015> Meeting Prof Lucas M. Schnorr                     :Herbstrith:
I was wrong, the algorithm is still a valid idea. My mistake was focusing on the word "Cache", while the algorithm does take advantage of
cache locality, it also does manage to do only half the calculations (thanks to Newtons third law, represented by the add_force function). 
* <08-12-2015> Code from the Decomposing a problem presentation on which we will work
 #+BEGIN_SRC c
 // calculate_forces kernel function ... here is where we start
__global__ void calculate_forces( int nbodies, Body *bodies)
{
  triangle(0, nbodies, bodies);
}
#+END_SRC
Not much to change here

#+BEGIN_SRC c

// triangle kernel function
__device__ void triangle(int n0, int n1, Body *bodies)
{
  int dn = n1 - n0;
  if(dn > 1){
    int nm = n0 + dn/2;

    triangle(n0, nm,bodies);  //spawn new thread
    triangle(nm,n1,bodies);

    rect(n0,nm,nm,n1,bodies);  //spawn new thread
  }
}
#+END_SRC

The triangle's and rect calls would only work on a dynamic parallelism capable device, else we
we cant spawn more threads to do the work.

#+BEGIN_SRC c

// rectangle kernel function ( without coarsening )
__device__ void rect(int i0, int i1, int j0, int j1, Body *bodies)
{
  int di = i1 - i0;
  int dj = j1 -j0;

  if(di > 1 && dj >1){
    int im = i0 + di/2;
    int jm = j0 + dj/2;

    rect(i0, im, j0, jm, bodies); //spawn new threads
    rect(im, i1, jm, j1, bodies);

    rect(i0, im, jm, j1, bodies); //spawn new threads
    rect(im, i1, j0, jm, bodies); //spawn new threads
    
  } else {
    if (di > 0 && dj >0){
      double fx, fy;
      calculate_force(&fx, &fy, bodies[i0], bodies[j0]);
      add_force(&bodies[i], fx, fy);
      add_force(&bodies[j], -fx, -fy);
    }
  }
}
#+END_SRC

Same as the triangle function

#+BEGIN_SRC c

//calculate_force: body x body interaction
__device__ void calculate_force(double *fx, double *fy, const Body &bi, const Body &bj)
{ 
  double dx = bj.x - bi.x;
  double dy = bj.y - bi.y;
  double dist2 = dx * dx + dy * dy;  //distance squared
  double dist = std::sqrt(dist2);
  double f = bi.mass * bj.mass * GRAVITY / dist2;
  *fx = f * dx / dist;
  *fy = f *dy / dist;
}

// add_force kernel function
__device__  void add_force(Body* b, double fx, double fy)
{
  b->xf += fx;
  b->yf += fy;
}

#+END_SRC
These shouldnt change much.

 What we would want is a proper way to fit these rectangles and triangles on the 
threads and thread blocks. This would give us an excelent boost on performance.
We also woll try the naive way by making use of the dynamic parallelism, but this
approach would have some overhead, and wouldn't have a good result as the first.
