#+TITLE: Herbstrith' Oblivious Nbody Lab Book
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Herbstrith(H) noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

* Study materials and theories
** Cuda
   What every CUDA Programmer Should Know About OpenGL
   http://www.nvidia.com/content/gtc/documents/1055_gtc09.pdf
   On Cuda usage of recursive kernels ( Dynamic Parallelism is only avaiable for devices with a compute capability >= 3.5)
   http://developer.download.nvidia.com/assets/cuda/files/CUDADownloads/TechBrief_Dynamic_Parallelism_in_CUDA.pdf
   http://devblogs.nvidia.com/parallelforall/cuda-dynamic-parallelism-api-principles/
   Classic
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
   Dynamic Parallelism simple example
   http://www.fz-juelich.de/SharedDocs/Downloads/IAS/JSC/EN/slides/advanced-gpu/adv-gpu-dynpar.pdf?__blob=publicationFile
   Cuda C Programming Guide
   http://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf  ( Appendix C, pg 140, for dynamic parallelism)

** Cache oblivious nbody
  + Presentation
    https://github.com/CppCon/CppCon2014/blob/master/Presentations/ -> Decomposing a problem

** GEMS3 nbody
http://http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html

* <03-12-2015> Algorithm study                                   :Herbstrith:
  One thing to note is that the algorithm idea is to have less RAM memory access, taking advantage on data locality.
  As GPU uses GDDR5 RAM, we won't have such an increase in performance as the cpu counterpart. Still data locality is a good thing to experiment on.
  We are also adding recursion to the algorithm, which the gpu doesn't quite like, and a thread seems to do more work, again because the algorithm was made thinking on cpus.
* <04-12-2015> Algorithm study                                   :Herbstrith:
An excerpt from the http://http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html article:
"
We may think of the all-pairs algorithm as calculating each entry f ij in an NxN grid of all pair-wise forces.[1] Then the total force F i
(or acceleration a i ) on body i is obtained from the sum of all entries in row i. Each entry can be computed independently, so there is O(N 2)
available parallelism. However, this approach requires O(N 2) memory and would be substantially limited by memory bandwidth. Instead, we serialize
some of the computations to achieve the data reuse needed to reach peak performance of the arithmetic units and to reduce the memory bandwidth required.

Consequently, we introduce the notion of a computational tile, a square region of the grid of pair-wise forces consisting of p rows and p columns. 
Only 2p body descriptions are required to evaluate all p 2 interactions in the tile (p of which can be reused later). These body descriptions can be
stored in shared memory or in registers. The total effect of the interactions in the tile on the p bodies is captured as an update to p acceleration
vectors.

To achieve optimal reuse of data, we arrange the computation of a tile so that the interactions in each row are evaluated in sequential order, updating
the acceleration vector, while the separate rows are evaluated in parallel.

"
This is a very similar approach to the one proposed in the cache-oblivious algorithm. On both we have the parallel tile notion, and both aim to make reuse of the data.
Also this one does take in account the gpu architecture.
* <07-12-2015> Meeting Prof Lucas M. Schnorr                     :Herbstrith:
I was wrong, the algorithm is still a valid idea. My mistake was focusing on the word "Cache", while the algorithm does take advantage of
cache locality, it also does manage to do only half the calculations (thanks to Newtons third law, represented by the add_force function). 
* <08-12-2015> Code from the Decomposing a problem presentation on which we will work
 #+BEGIN_SRC c
 // calculate_forces kernel function ... here is where we start
__global__ void calculate_forces( int nbodies, Body *bodies)
{
  triangle(0, nbodies, bodies);
}
#+END_SRC
Not much to change here

#+BEGIN_SRC c

// triangle kernel function
__device__ void triangle(int n0, int n1, Body *bodies)
{
  int dn = n1 - n0;
  if(dn > 1){
    int nm = n0 + dn/2;

    triangle(n0, nm,bodies);  //spawn new thread
    triangle(nm,n1,bodies);

    rect(n0,nm,nm,n1,bodies);  //spawn new thread
  }
}
#+END_SRC

The triangle's and rect calls would only work on a dynamic parallelism capable device, else we
we cant spawn more threads to do the work.

#+BEGIN_SRC c

// rectangle kernel function ( without coarsening )
__device__ void rect(int i0, int i1, int j0, int j1, Body *bodies)
{
  int di = i1 - i0;
  int dj = j1 -j0;

  if(di > 1 && dj >1){
    int im = i0 + di/2;
    int jm = j0 + dj/2;

    rect(i0, im, j0, jm, bodies); //spawn new threads
    rect(im, i1, jm, j1, bodies);

    rect(i0, im, jm, j1, bodies); //spawn new threads
    rect(im, i1, j0, jm, bodies); //spawn new threads
    
  } else {
    if (di > 0 && dj >0){
      double fx, fy;
      calculate_force(&fx, &fy, bodies[i0], bodies[j0]);
      add_force(&bodies[i], fx, fy);
      add_force(&bodies[j], -fx, -fy);
    }
  }
}
#+END_SRC

Same as the triangle function

#+BEGIN_SRC c

//calculate_force: body x body interaction
__device__ void calculate_force(double *fx, double *fy, const Body &bi, const Body &bj)
{ 
  double dx = bj.x - bi.x;
  double dy = bj.y - bi.y;
  double dist2 = dx * dx + dy * dy;  //distance squared
  double dist = std::sqrt(dist2);
  double f = bi.mass * bj.mass * GRAVITY / dist2;
  *fx = f * dx / dist;
  *fy = f *dy / dist;
}

// add_force kernel function
__device__  void add_force(Body* b, double fx, double fy)
{
  b->xf += fx;
  b->yf += fy;
}

#+END_SRC
These shouldnt change much.

 What we would want is a proper way to fit these rectangles and triangles on the 
threads and thread blocks. This would give us an excelent boost on performance.
We also woll try the naive way by making use of the dynamic parallelism, but this
approach would have some overhead, and wouldn't have a good result as the first.
* <10-12-2015> Cuda Implementation
** Dynamic Parallelism
 We shall start by making a version that makes use of the Dinamyc Parallelism, which should be easier to implement.
 This version will be very similar to the Cache Oblivious implementation, so we might not have as much as an increase
 performance.
 We will represent the data in a similar fashion to the GEMS3 nbody, so we can compare to it on a first analisys.
** Cuda Native
 This version we will try to take a new approach to the algorithm, taking in consideration the GPU architecture.
 One first idea is to move diagonally in the grid at each iteration. This would lead to a low work load on the first
 iterations and increase it as the computation goes. We could also think of ways of getting work from the "middle"
 tiles, trying to have more  work on the first iterations.
 
* <19-12-2015> Running on Orion
 Some many problems appeared here.
 In the end, to compile i used the following commands:
 nvcc -arch=sm_35 -rdc=true -c nbody_kernel.cu
 nvcc -arch=sm_35 -dlink -o nbody_kernel_link.o nbody_kernel.o -lcudadevrt -lcudart
 g++ nbody_kernel.o nbody_kernel_link.o main.cpp -L/usr/local/cuda/lib64/ -lcudart -lcudadevrt

 Also needed to set export LD_LIBRARY_PATH=/usr/local/cuda-7.0/lib64:$LD_LIBRARY_PATH
 But still got the following error: FATAL: Error inserting nvidia_340_uvm : invalid argument

 After some more cuda exploring, i managed to run a dynamic paralel example on my gtx 760. So i can
 use my machine for compiling and running the code ( i tought the error's i had received were due to
 the gtx 760 not having a  3.5 cc). This same code would generate the same error as mentioned before
 on the orion machine. Now we need to fix this.
 
 And an approach to compile using just a Makefile can was found here:
 http://stackoverflow.com/questions/28719927/dynamic-parallelism-separate-compilation-undefined-reference-to-cudaregiste
